---
title: 'ESS 330 Final Project Report - Lab 9'
authors:
  - name: Bella Conrad
    affiliation: Colorado State University
    roles: 
      - writing
    corresponding: false
  - name: Zachary Cramton
    affiliation: Colorado State University
    roles: 
      - writing
      
    corresponding: true
  - name: Rachel Delorie
    affiliation: Colorado State University
    roles: 
      - writing
    corresponding: false
bibliography: references.bib
csl: apa.csl
---

### NOTE: Rendering is currently broken, just save and push. I will work on it with Mike next week.

# Urbanization, Density and Access to Public Parks in the United States

## Abstract

### Introduction

<p> Since 2008, the majority of the world's population has lived in urban areas, a result of urbanization in developing countries [@kohlhase2013new; @beall2010urbanization]. The United States developed earlier than many nations, with more than 50 percent of the population living in urban areas. 

:::{.callout-note}
### Definition of Urban Areas in Census History
Prior to the 2020 Census urban areas were defined as any area with greater than 2500 people. Following the 2010 census urban clusters described areas with populations greater than 2,500 and less than 50,000; urbanized areas described areas with a population greater than 50,000. For the 2020 Census the threshold was changed to 5000 people [@ratcliffe_redefining_2022].
:::

<p> by the 14th Census in 1920. In the century since the 1920 census the percentage of individuals living in urban areas has increased to 80.7% [@slack2020changing]. As more people moved to urban areas, those areas expanded forming urbanized areas and large cities.
<p>  Urban planning has existed for centuries out of necessity, historically dominated by efficiency and utilitarianism, optimizing the world we live in for profitability, corporate productivity, and automobile-based mobility. This optimization came with sacrifices, which now impact an increasingly large majority of the population. In recent years, the discipline has begun to prioritize human factors over utilitarian efficiency. Thousands of years of living in rural settings makes urban living hard for most people's biology. Connection to nature and time outdoors even in small amounts has been shown to be a vital part maintaining physical and mental health [DOTHIS:: Find additional source]. In an effort to make urban spaces more livable, planners are turning to parks and natural areas to connect people to nature.
<p>  Equity issues aside, overturning and correcting more than a century of bad planning is a daunting task. Many cities filled in and built up over the course of the 20th century as land became a premium commodity [DOTHIS:: Find additional source]. Does this density present significant challenges for today's planning professionals? This research seeks to investigate the relationship between urban demographics like density and park access. In exploring this relationship, we hypothesize that there is an intermediate/sublinear relationship between urban population density and public open space availability.

### Data Overview

<p> This report uses data from the UN-Habitat Urban Indicators Database and the ParkServe® Database maintained by the Trust for Public Land. The UN data relates to the UN-SDG 11.7.1 pertaining to access to open spaces and green areas. 
<p> The January 2025 version of the UN Open Spaces and Green Areas data includes the average share of urban areas allocated to streets and open public spaces as well as the share of the urban population with convenient access to an open public space. 

::: {.callout-note}
### UN Definition
In this case, the UN defines "convenient access to an open public space" as the "urban population withing 400 meters walking distance along the street network to an open public space" [@may2000habitat].
:::

<p> These data collected by the UN were collected in 2020 and provided as a .xls format spreadsheet. These data were converted to .csv format with Microsoft Excel. The ParkServe® data selected for use is the 2020 data set to match the year the UN data was recorded. Specifically, this report uses elements of the City Park Facts: Acreage & Park System Highlights. The ParkServe® data is much less synthesized and was available as a .xml file. The file was structured for viewing as a spreadsheet rather than for further analysis and included multiple worksheets withing the workbook. In converting the file to a .csv file, the data spread across multiple worksheets was collated in a single worksheet and converted to a summarized dataset .csv file.
<p> These data are lacking a shared numerical position data type but share a city name column formatted as "city_name, two_letter_state_abbreviation". There is not perfect overlap between cities with data in each database however, there are 25 cities shared between the datasets. Cities present in only one data set will be culled when the data is joined.

### Methods
1.	Clean the data. The raw data were downloaded as Excel spreadsheets, some reformatting in Excel was required to effectively exporting as a .csv file and importing the new summarized file to RStudio. Remaining data cleaning will occur in R as needed including any header changes or additional columns needed.
2.	Conduct Exploratory Data Analysis (EDA). 
3.	Join datasets by “city name” to have a complete working dataset. These data will be combined into a single data frame with an inner join because there is a large number of cities listed in one data set but not the other. The new dataset will include only cities found in both datasets, with columns from both.

::: {.callout-note}
## Limiting Scope
The cities found in only one dataset will be cut from the data to accommodate the limited scope of the project. With a bigger scope it is possible that additional data could be used to understand these patterns with more depth.
:::

4.	Prep data and split it into training and testing datasets. Perform a 10-fold cross-validation on training data. 
5.	Create a recipe.
6.	Set up several models in regression mode. 
7.	Create a workflow set including the previously written models and the recipe. 
8.	Map function over workflow using workflow map.
9.	Using the highest performing model, fit the data and augment. 
10.	Plot and graph data to visually display test results.
11. Explore using the model to predict values for cities included in only one document (if time allows).

## Exploratory Data Analysis (EDA)
<p> The data has already been discussed in general terms in the data overview section. There will be a readme file created to elaborate on the sources, formatting and manipulation required for each dataset before joining them into the urban_parks_data data frame. In general terms, prior to importing into RStudio, the .xml files the data came in were opened in MS Excel; the sheets were formatted to be converted to .csv files including condensing multiple worksheets of the ParkServe® data into a sinigle sheet for easier conversion to a .csv file. While some of the cleaning done in Excel could have been completed in RStudio, it was not efficient to do so. Using Excel was faster and more flexible for that use case. Similar reformatting was required with the UN data as the headers were unreasonably long by default. The readme that will be created for each (or both) files will include a more detailed summary of what each variable means.

```{r}
#| label: Data and Library Setup

library(baguette)       # Model bagging methods for ensemble learning
library(flextable)      # Tool for building display-ready tables
library(ggfortify)      # Autoplot support for time series, PCA, and more
library(ggthemes)       # Extra themes and scales for ggplot2
library(glmnet)         # Linear model good with small sample size and minimal overfitting
library(glue)           # String interpolation with embedded R expressions
library(here)           # Simplifies file paths for project-based workflows
library(parsnip)        # Unified interface to model functions (part of tidymodels)
library(patchwork)      # Combine ggplot2 plots with intuitive syntax
library(plotly)         # Interactive plots built on top of ggplot2 or base R
library(powerjoin)      # Flexible joins that track and resolve conflicts
library(purrr)          # For functional programming (mapping)
library(recipes)        # Preprocessing pipelines for modeling (part of tidymodels)
library(skimr)          # Quick summaries of data frames with descriptive stats
library(tidymodels)     # Collection of packages for modeling and machine learning
library(tidyverse)      # Core packages for data science: dplyr, ggplot2, readr, etc.
library(tune)           # Tools for hyperparameter tuning (part of tidymodels)
library(vip)            # Variable importance plots for many model types
library(visdat)         # Visualize missing and data types in your dataset
library(workflowsets)   # Manage and compare sets of modeling workflows
library(xgboost)        # Fast, regularized gradient boosting machine
library(yardstick)      # Metrics for evaluating model performance (part of tidymodels)

# Import data from csvs and clean NAs
parkserve_data <- read_csv(here("data", "clean_data/parkserve_summarized_facts_2020.csv")) %>% 
  drop_na()
un_land_use_data <- read_csv(here("data", "clean_data/un_land_use.csv")) %>% 
  drop_na()

# Add columns and finish cleaning parkserve data
clean_parkserve_data <- parkserve_data %>%
  mutate(
    across(-city_name, ~ as.numeric(.x)),  # Convert all columns to numeric except for city name
    parkland_percent = parkland_percent * 100, # Convert parkland percent from ratio
    # Fix design/natural park area percentage calculations
    percent_designed_parks = ifelse(parkland_area == 0, NA, (designed_park_area / parkland_area) * 100),
    percent_natural_parks = ifelse(parkland_area == 0, NA, (natural_park_area / parkland_area) * 100),
    #New calculations
    dn_area_ratio = ifelse(percent_natural_parks == 0, NA, percent_designed_parks / percent_natural_parks),   # Designed-natural area ratio
    parkland_per_capita = ifelse(city_pop == 0, NA, parkland_area / city_pop),   # Parkland per capita
    land_per_capita = ifelse(city_pop == 0, NA, land_area / city_pop),   # Land per capita (opposite of density)
    pop_near_parks = city_pop * percent_half_mile_walk,   # Pop near parks (very similar to UN data open space access, DOTHIS:: If time compare pop_near_parks and un open space access)
    park_units_per_area = ifelse(land_area == 0, NA, park_units / land_area),   # Park unit density
    park_units_per_capita = ifelse(city_pop == 0, NA, park_units / city_pop)   # Park units per capita
  )
  
# Join data removing cities found in only one of the two datasets
urban_parks_data <- clean_parkserve_data %>% 
  inner_join(un_land_use_data, by = "city_name")  

# Basic data structure exploration
glimpse(urban_parks_data)
```


```{r}
#| label: Basic EDA

# Descriptive Stats

  # Write function to round numeric columns to two decimal places
  round_numeric <- function(df) {
    df %>% 
      mutate(across(where(is.numeric), ~round(.x, 2)))
  }

  # Summarize stats by variable
    desc_stats_parks <- urban_parks_data %>% 
      select(where(is.numeric)) %>% 
      pivot_longer(cols = everything(), names_to = "variable", values_to = "value") %>% 
      group_by(variable) %>% 
      summarize(mean = mean(value, na.rm = TRUE), 
                median = median(value, na.rm = TRUE), 
                sd = sd(value, na.rm = TRUE),
                Q1 = quantile(value, 0.25, na.rm = TRUE),
                Q3 = quantile(value, 0.75, na.rm = TRUE)) %>% 
      round_numeric()
      
      # Print descriptive stats with flextable
      desc_stats_flex <- flextable(desc_stats_parks) %>%
        set_caption("Summarized Urban Parks Statistics") %>% 
        set_header_labels(
          variable = "Variable",
          mean = "Mean",
          median = "Median",
          sd = "Standard Deviation",
          Q1 = "1st Quartile (Q1)",
          Q3 = "3rd Quartile (Q3)") %>% 
        autofit()

# Find Top/Bottom cities for percent parkland
  
  # Select relevant columns
    simplified_vars <- c("city_name", "city_pop", "revised_area", "pop_density", "parkland_area", "dn_area_ratio", "parkland_percent", "parkland_per_1k_pop", "percent_half_mile_walk", "pop_near_parks")
  
  # Filter top/bottom 10 cities
  top10_park_percent <- urban_parks_data %>% 
    arrange(desc(parkland_percent)) %>% 
    slice_head(n = 10) %>% 
    select(all_of(simplified_vars)) %>% 
    round_numeric()
  
  bottom10_park_percent <- urban_parks_data %>% 
    arrange(parkland_percent) %>% 
    slice_head(n = 10) %>% 
    select(all_of(simplified_vars)) %>% 
    round_numeric()
  
  # Create top/bottom 10 flextables w/ function
  # Create function
  make_best_worst_flextbl <-function(df, caption) {
    flextable(df) %>% 
    set_caption(caption) %>%
    set_header_labels(
      city_name = "City Name",
      city_pop = "City Population",
      revised_area = "City Land Area (Revised) (Acres)",
      pop_density = "Population Density (People/Acre)",
      parkland_area = "City Parkland Area (Acres)",
      parkland_percent = "Percent Parkland",
      parkland_per_1k_pop = "Parkland Per (1000) Capita",
      percent_half_mile_walk = "Percent of Residents within 0.5 Miles of a Park",
      dn_area_ratio = "Designed-Natural Park Area Ratio (Designed Park (%) / Natural Park (%)") %>% 
    autofit()
  }
  
  top10_park_percent_flex <- make_best_worst_flextbl(top10_park_percent, "Top 10 Cities for Parkland Percentage")
  
  bottom10_park_percent_flex <- make_best_worst_flextbl(bottom10_park_percent, "Top 10 Cities for Parkland Percentage")
  
# Make plots to visualize the data
# Histogram: Land Area
land_area_plot <- ggplot(urban_parks_data, aes(x = as.numeric(land_area))) +
  geom_histogram(bins = 20, fill = "steelblue", color = "white") +
  labs(x = "Land Area (Acres)", y = "Frequency", title = "Distribution of City Land Areas") +
  theme_minimal()

# Scatterplot: Land Area vs Parkland Area
land_vs_park_area_plot <- ggplot(urban_parks_data, aes(x = as.numeric(land_area), y = parkland_area)) +
  geom_point(color = "forestgreen") +
  labs(x = "Land Area (Acres)", y = "Parkland Area\n(Acres)", title = "Land Area vs Parkland Area") +
  scale_x_continuous(labels = scales::label_comma()) +
  scale_y_continuous(labels = scales::label_comma()) +
  theme_minimal()

# Scatterplot: Population Density vs Parkland Percent
density_vs_park_percent_plot <- ggplot(urban_parks_data, aes(x = as.numeric(pop_density), y = parkland_percent)) +
  geom_point(color = "darkorange") +
  labs(x = "Population Density (People/Acre)", y = "Percent Parkland", title = "Population Density vs\nParkland Percent") +
  scale_x_continuous(labels = scales::label_comma()) +
  scale_y_continuous(labels = scales::label_comma()) +
  theme_minimal()

# Scatterplot: Designed-Natural Park Area Ratio vs Parkland Percent
dn_area_ratio_vs_park_percent_plot <- ggplot(urban_parks_data, aes(x = dn_area_ratio, y = parkland_percent)) +
  geom_point(color = "mediumvioletred") +
  labs(x = "Designed-Natural Park Area Ratio", y = "Percent Parkland", title = "Designed-Natural Park Area\nRatio vs Parkland Percent") +
  scale_x_continuous(labels = scales::label_comma()) +
  scale_y_continuous(labels = scales::label_comma()) +
  theme_minimal()

# Scatterplot: Population Near Parks vs Mean % Open Space Access"
pop_park_proximity_plot <- ggplot(urban_parks_data, aes(x = mean_percent_open_space_access, y = (pop_near_parks/100000))) +
  geom_point(color = "cadetblue") +
  labs(x = "Mean % Open Space Access", y = "100k Population\nNear Parks", title = "Population Near Parks vs\nMean % Open Space Access") +
  theme_minimal()

# Scatterplot: Percent Open Space Access vs Percent Built Open Space
open_space_access_vs_built_plot <- ggplot(urban_parks_data, aes(x = mean_percent_open_space_access, y = mean_percent_built_open_space)) +
  geom_point(color = "seagreen") +
  labs(x = "Mean % Open Space Access", y = "Mean % Built\nOpen Space", title = "Accessibility of Built Open Space") +
  theme_minimal()

# Scatterplot: Land Per Capita vs Parkland Area
land_pc_vs_park_area_plot <- ggplot(urban_parks_data, aes(x = land_per_capita, y = parkland_area)) +
  geom_point(color = "royalblue") +
  labs(x = "Land Per Capita (Acres)", y = "Parkland Area (Acres)", title = "Land Per Capita vs Parkland Area") +
  theme_minimal()

# Scatterplot: Parkserve Proxomity Data vs\nUN Access Data
pct_walk_vs_osa_plot <- ggplot(urban_parks_data, aes(x = percent_half_mile_walk, y = mean_percent_open_space_access)) +
  geom_point(color = "darkred") +
  labs(x = "% of Pop. < 0.5 Miles\nfrom a Park", y = "Mean % Open\nSpace Access", title = "Park Proximity (Parkserve)\n vs Access (UN)") +
  theme_minimal()

# Use corr plots to show variables with strong impact on model outputs
  # Function to create a corr plot for a given target variable
  create_corr_plot <- function(data, target_var) {
    
    # Check if target_var exists in the dataset
    if (!(target_var %in% names(data))) {
      stop("Target variable not found in the dataset!")
    }
    
    # Compute correlation of all numeric variables with the target variable
    correlation_matrix <- cor(data %>% select_if(is.numeric), use = "complete.obs")  # Compute correlation matrix
    
    # Extract the correlation values for the target variable and convert to data frame
    correlation_data <- data.frame(
      variable = rownames(correlation_matrix),
      correlation = abs(correlation_matrix[target_var, ])
    ) %>%
        filter(variable != target_var) %>%  # Remove the target variable itself
        arrange(desc(correlation))  # Sort by correlation value
    
    # Create corr plot
    corr_plot <- ggplot(correlation_data, aes(x = reorder(variable, correlation), y = correlation)) +
      geom_bar(stat = "identity", fill = "steelblue") +
      coord_flip() +  # Flip coordinates to create horizontal bars
      labs(x = "Variables", y = "Absolute Correlation", title = paste("Feature Correlation\nw/", target_var)) +
      theme_minimal()
    
    return(corr_plot)
  }

# Make plots
bos_corr_plot <- create_corr_plot(urban_parks_data, "mean_percent_built_open_space")
osa_corr_plot <- create_corr_plot(urban_parks_data, "mean_percent_open_space_access")

# Combine all plots in one figure using patchwork (optional)
pt1_eda_plots <- (land_area_plot | land_vs_park_area_plot) / 
  (density_vs_park_percent_plot | land_pc_vs_park_area_plot) +
    plot_layout(guides = "collect")

pt2_eda_plots <- (dn_area_ratio_vs_park_percent_plot | pct_walk_vs_osa_plot) / 
  (pop_park_proximity_plot | open_space_access_vs_built_plot) +
    plot_layout(guides = "collect")
  
# Display data summary and visualization

  # Display flextables
  desc_stats_flex
  top10_park_percent_flex
  bottom10_park_percent_flex 
  
  # Display patchwork plots
  pt1_eda_plots
  pt2_eda_plots
  bos_corr_plot
  osa_corr_plot

```

## Modeling

### 4.	Prep data and split it into training and testing datasets. Perform a 10-fold cross-validation on training data. 
```{r Data Splitting}
# Set a random seed for reproducibility
set.seed(567)

# Split data into training (80%) and testing (20%) sets
up_split <- initial_split(urban_parks_data, prop = 0.8)
up_train <- training(up_split)
up_test <- testing(up_split)

# Create bootstraps resampling for the training data
up_boot_splits <- bootstraps(data = up_train, times = 25) # Using bootstraps stabilizes the error estimate with many model evals

```

### 5.	Create recipes with built open space and open space access as outputs.
```{r Preprocessing Recipes}
# Recipe for mean_percent_built_open_space
rec_bos <- recipe(mean_percent_built_open_space ~ parkland_per_capita + designed_park_area + land_per_capita + natural_park_area + parkland_area, data = up_train) %>%
  #step_nzv(all_predictors()) %>% # Remove predictors with near zero variance to prevent bloated fits
  #step_lincomb(all_predictors()) %>% # Removes linear combinations of predictors (redundancy)
  step_YeoJohnson(all_predictors()) %>%  # Apply Yeo-Johnson transformation to numeric variables
  step_corr(all_predictors(), threshold = 0.9)  # Remove highly correlated predictors (threshold > 0.9)

# Recipes for mean_percent_open_space_access
rec_osa_1 <- recipe(mean_percent_open_space_access ~ parkland_percent + percent_half_mile_walk + pop_density + parkland_area + pop_near_parks + parkland_per_capita + city_pop, data = up_train) %>%
    # Add interaction terms
  step_interact(terms = ~ parkland_percent:pop_density) %>%
  step_interact(terms = ~ parkland_percent:percent_half_mile_walk) %>%
  step_interact(terms = ~ pop_density:percent_half_mile_walk) %>% 
  #step_nzv(all_predictors()) %>% # Remove predictors with near zero variance to prevent bloated fits
  #step_lincomb(all_predictors()) %>% # Removes linear combinations of predictors (redundancy)
  step_YeoJohnson(all_predictors()) %>%  # Apply Yeo-Johnson transformation to numeric variables
  step_corr(all_predictors(), threshold = 0.9)  # Remove highly correlated predictors (threshold > 0.9)


rec_osa_2 <- recipe(mean_percent_open_space_access ~ parkland_percent + percent_half_mile_walk + pop_density + parkland_area + pop_near_parks + parkland_per_capita + city_pop, data = up_train) %>%
  #step_nzv(all_predictors()) %>% # Remove predictors with near zero variance to prevent bloated fits
  #step_lincomb(all_predictors()) %>% # Removes linear combinations of predictors (redundancy)
  step_YeoJohnson(all_predictors()) %>%  # Apply Yeo-Johnson transformation to numeric variables
  step_corr(all_predictors(), threshold = 0.9)  # Remove highly correlated predictors (threshold > 0.9)
  
```

### 6.	Set up several models in regression mode. 
```{r Model Specifications}
# Linear Regression
lasso_model <- linear_reg() %>%
  set_engine("glmnet") %>% 
  set_mode("regression")

# Random Forest (Tree-Based Model)
rf_model <- rand_forest() %>%
  set_engine("ranger") %>%
  set_mode("regression")

# Gradient Boosting Machine (Boosted Tree Model)
# More aggressive regularization to prevent over fitting
xg_model <- boost_tree() %>%
   set_engine("xgboost") %>%
  set_mode("regression")

```

### 7. Compose workflow sets to test each recipe on all models.
```{r Workflow setup}
# Compile list of models
models <- list(lasso = lasso_model, rf = rf_model, xg = xg_model)
# Compile one recipe list for each output
bos_recipes <- list(bos = rec_bos)
osa_recipes <- list(osa_1 = rec_osa_1, osa_2 = rec_osa_2)

# Create workflow set for BOS
bos_wf_set <- workflow_set(
  preproc = bos_recipes, 
  models = models) %>% 
  workflow_map('fit_resamples', resamples = up_boot_splits)

# Create workflow set for OSA
osa_wf_set <- workflow_set(
  preproc = osa_recipes, 
  models = models) %>% 
  workflow_map('fit_resamples', resamples = up_boot_splits)

```
### 8. Vizualize Model Performance
####   Rank and Select the Best Model for Each Output
```{r Visualize Model Performance & Ranking}
# Visualize Model Performance
autoplot(bos_wf_set) + 
  aes(x = wflow_id) + 
  labs(title = "BOS Resample Metrics",
       x = "Model") +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 45, hjust = 1))

autoplot(osa_wf_set) + 
  aes(x = wflow_id) +
  labs(title = "OSA Resample Metrics",
       x = "Model") + 
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 45, hjust = 1))

# Rank Results
rank_results(bos_wf_set, rank_metric = "rsq", select_best = TRUE)
rank_results(osa_wf_set, rank_metric = "rsq", select_best = TRUE)
```

```{r Test Tuning}
# Tune BOS Model
tuned_bos_model <- boost_tree(trees = tune(), 
                              min_n = tune(),
                              learn_rate = tune(),
                              sample_size = tune()) %>%
  set_engine("xgboost") %>% 
  set_mode("regression")
  
bos_tuned_wf <- workflow() %>% 
  add_recipe(rec_bos) %>% 
  add_model(tuned_bos_model)

bos_dials <- extract_parameter_set_dials(bos_tuned_wf)
bos_grid <- grid_space_filling(bos_dials, size = 20)

bos_model_params <- tune_grid(
  bos_tune_wf,
  resamples = up_boot_splits,
  grid = bos_grid,
  metrics = metric_set(rmse, rsq, mae),
  control = control_grid(save_pred = TRUE)
)

autoplot(bos_model_params)

collect_metrics(bos_model_params)
best_mae <- show_best(bos_model_params, metric = "rsq", n = 1)
hp_best <- select_best(bos_model_params, metric = "rsq")

bos_final_wf <- finalize_workflow(bos_tune_wf, hp_best)
bos_final_fit <- last_fit(bos_final_wf, split = up_split)
bos_final_metrics <- collect_metrics(bos_final_fit)

bos_predictions <- collect_predictions(bos_model_params)

ggplot(bos_predictions, aes(x = .pred, y = mean_percent_built_open_space)) +
  geom_smooth(method = lm, color = "blue") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  scale_color_gradient() +
  labs(
    title = "Actual vs. Predicted Values", 
    x = "Predicted", 
    y = "Actual")

bos_final_fit_full <- fit(bos_final_wf, data = camels)
bos_augmented_preds <- augment(bos_final_fit_full, new_data = urban_parks_data)

bos_augmented_preds <- bos_augmented_preds %>% 
  mutate(residual_sq = (mean_percent_built_open_space - .pred)^2)

bos_map_preds <- ggplot(bos_augmented_preds, aes(x = .pred, y = mean_percent_built_open_space)) +
  geom_point(aes(color = .pred), size = 3, alpha = 0.8) +
  scale_color_viridis_c(name = "Predicted") +
  coord_fixed() +
  labs(title = "Map of Predicted Mean % Built Open Space") +
  theme_minimal()

bos_map_resid <- ggplot(bos_augmented_preds, aes(x = .pred, y = residual_sq)) +
  geom_point() +
  scale_color_viridis_c(name = "Residual²") +
  labs(title = "Map of Squared Residuals") +
  theme_minimal()

bos_maps_combined <- bos_map_preds | bos_map_resid


# Tune OSA Model
tuned_osa_model <- rand_forest(mtry = tune(),
                              trees = tune(),
                              min_n = tune()) %>% 
  set_engine("ranger") %>% 
  set_mode("regression")
  
osa_tuned_wf <- workflow() %>% 
  add_recipe(rec_osa_1) %>% 
  add_model(tuned_osa_model)

osa_dials <- extract_parameter_set_dials(osa_tuned_wf)
osa_grid <- grid_space_filling(osa_dials, size = 20)

osa_model_params <- tune_grid(
  osa_tune_wf,
  resamples = up_boot_splits,
  grid = osa_grid,
  metrics = metric_set(rmse, rsq, mae),
  control = control_grid(save_pred = TRUE)
)

autoplot(osa_model_params)

collect_metrics(osa_model_params)
best_mae <- show_best(osa_model_params, metric = "rsq", n = 1)
hp_best <- select_best(osa_model_params, metric = "rsq")

osa_final_wf <- finalize_workflow(osa_tune_wf, hp_best)
osa_final_fit <- last_fit(osa_final_wf, split = up_split)
osa_final_metrics <- collect_metrics(osa_final_fit)

osa_predictions <- collect_predictions(osa_model_params)

ggplot(osa_predictions, aes(x = .pred, y = mean_percent_open_space_access)) +
  geom_smooth(method = lm, color = "blue") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  scale_color_gradient() +
  labs(
    title = "Actual vs. Predicted Values", 
    x = "Predicted", 
    y = "Actual")

osa_final_fit_full <- fit(osa_final_wf, data = camels)
osa_augmented_preds <- augment(osa_final_fit_full, new_data = urban_parks_data)

osa_augmented_preds <- osa_augmented_preds %>% 
  mutate(residual_sq = (mean_percent_open_space_access - .pred)^2)

osa_map_preds <- ggplot(osa_augmented_preds, aes(x = .pred, y = mean_percent_open_space_access)) +
  geom_point(aes(color = .pred), size = 3, alpha = 0.8) +
  scale_color_viridis_c(name = "Predicted") +
  coord_fixed() +
  labs(title = "Map of Predicted Mean % Built Open Space") +
  theme_minimal()

osa_map_resid <- ggplot(osa_augmented_preds, aes(x = .pred, y = residual_sq)) +
  geom_point() +
  scale_color_viridis_c(name = "Residual²") +
  labs(title = "Map of Squared Residuals") +
  theme_minimal()

osa_maps_combined <- osa_map_preds | osa_map_resid

print(bos_maps_combined)
print(osa_maps_combined)
```
















------------------------------
#Example:
  
#fit_test <- workflow() %>% 
# add_model(xg_model) %>% 
# add_recipe(rec_bos) %>% 
# fit(up_train)

# predictions_test <- augment(fit_test, new_data = up_train) %>%
  # mutate(diff = abs(mean_percent_built_open_space - .pred))

# pred_met_test <- metrics(predictions_test, truth = mean_percent_built_open_space, estimate = .pred)
------------------------

7.	Create a workflow set combining recipes and models into a workflow set for cross validation.
```{r Workflow Set Modeling}
# Create a workflow set combining recipes and models, fit resamples
bos_wf_set <- workflow_set(
  preproc = list(bos = rec_bos),
  models = models) %>% 
  workflow_map('fit_resamples', resamples = up_10cv)

osa_wf_set <- workflow_set(
  preproc = list(osa_1 = rec_osa_1, osa_2 = rec_osa_2),
  models = models) %>% 
  workflow_map('fit_resamples', resamples = up_10cv)

# Visualize model performance
autoplot(bos_wf_set) + 
  aes(x = wflow_id) + 
  labs(title = "BOS Resample Metrics",
       x = "Model") +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 45, hjust = 1))

autoplot(osa_wf_set) + 
  aes(x = wflow_id) +
  labs(title = "OSA Resample Metrics",
       x = "Model") + 
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 45, hjust = 1))

# Rank the models based on R-squared metric
rank_results(bos_wf_set, rank_metric = "rsq", select_best = TRUE)
rank_results(osa_wf_set, rank_metric = "rsq", select_best = TRUE)

```





8. Tune the best model for each recipe to optimize hyperparameters
```{r Tuning Models}
# Tune the best model for BOS
tuned_bos_model <- model_func() %>% 
  set_engine("model_engine") %>% 
  set_mode("regression")

wf_bos_tune <- workflow %>% 
  add_recipe() %>% 
  add_model(tuned_bos_model)

bos_dials <- extract_parameter_set_dials(wf_bos_tune)
bos_grid <- (grid_space_filling(bos_dials, size = 20))

bos_model_params <- tune_grid(
  wf_bos_tune,
  resamples = up_10cv,
  grid = bos_grid,
  metrics = metric_set(rmse, rsq, mae),
  control = control_grid(save_pred = TRUE)
)

# Tune the best model for OSA
tuned_osa_model <- model_func() %>% 
  set_engine("model_engine") %>% 
  set_mode("regression")

wf_osa_tune <- workflow %>% 
  add_recipe() %>% 
  add_model(tuned_osa_model)

osa_dials <- extract_parameter_set_dials(wf_osa_tune)
osa_grid <- (grid_space_filling(osa_dials, size = 20))

osa_model_params <- tune_grid(
  wf_osa_tune,
  resamples = up_10cv,
  grid = osa_grid,
  metrics = metric_set(rmse, rsq, mae),
  control = control_grid(save_pred = TRUE)
)

```

9. Visualize and rank model performance of tuned models
```{r Model Performance Visualization and Ranking}
# Plot the tuning results for model comparison
autoplot(bos_model_params) + 
  ggtitle("BOS Tuning Results") +
    aes(x = wflow_id) + 
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 45, hjust = 1))


autoplot(osa_model_params) + 
  ggtitle("OSA Tuning Results") +
  aes(x = wflow_id) +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 45, hjust = 1))

# Rank the models based on R-squared metric
rank_results(bos_model_params, rank_metric = "rsq", select_best = TRUE)
rank_results(osa_model_params, rank_metric = "rsq", select_best = TRUE)
```






------------------------------------




8.	Tune models to optimize hyperparameters for each.
```{r Model Tuning}
# Tune Models for bos_wf_set
bos_tuned_wf_set <- workflow_map(
    bos_wf_set,
    fn = "tune_grid",
    resamples = up_10cv,
    grid = 10,  # Specify grid size or tuning parameters
    metrics = metric_set(rmse, rsq, mae),
    control = control_grid(save_pred = TRUE)
  )

# Tune Models for osa_wf_set
osa_tuned_wf_set <- workflow_map(
    osa_wf_set,
    fn = "tune_grid",
    resamples = up_10cv,
    grid = 10,  # Specify grid size or tuning parameters
    metrics = metric_set(rmse, rsq, mae),
    control = control_grid(save_pred = TRUE)
  )

bos_model_parameters <- collect_metrics(bos_tuned_wf_set)
osa_model_parameters <- collect_metrics(osa_tuned_wf_set)
```

9. Visualize and rank model performance of tuned models
```{r Model Performance Visualization and Ranking}
# Plot the tuning results for model comparison
autoplot(bos_tuned_wf_set) + 
  ggtitle("BOS Tuning Results") +
    aes(x = wflow_id) + 
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 45, hjust = 1))


autoplot(osa_tuned_wf_set) + 
  ggtitle("OSA Tuning Results") +
  aes(x = wflow_id) +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 45, hjust = 1))

# Rank the models based on R-squared metric
rank_results(bos_tuned_wf_set, rank_metric = "rsq", select_best = TRUE)
rank_results(osa_tuned_wf_set, rank_metric = "rsq", select_best = TRUE)

# Rank and Select best models based on rank_metric
# Select ranking metric
rank_metric = "rsq"

# Select best models
bos_best_params <- bos_model_parameters %>% 
  ?select_best(rank_metric)

osa_best_params <- osa_model_parameters %>% 
  select_best(rank_metric)

```

9.	Select the highest performing model for each output to perform a final fit.
```{r Custom functions for model finalization and eval.}
# Function to finalize the model using the best hyperparameters
finalize_model <- function(model_params, workflow, split, metric = "mae") {
  # Select the best hyperparameters
  hp_best <- select_best(model_params, metric)
  
  # Finalize the workflow with the best hyperparameters
  final_wf <- finalize_workflow(workflow, hp_best)
  
  # Fit the finalized workflow to the training data
  final_fit <- last_fit(final_wf, split = split)
  
  return(final_fit)
}

# Function to augment predictions and calculate residuals
augment_with_residuals <- function(final_fit, new_data, target_var) {
  augment_preds <- augment(final_fit, new_data = new_data) %>%
    mutate(residual_sq = (get(target_var) - .pred)^2)  # Squared residuals
  
  return(augment_preds)
}

# Function to plot predictions vs actual values and residuals
plot_predictions <- function(augmented_preds, target_var, title_prefix = "Model") {
  p1 <- ggplot(augmented_preds, aes(x = .pred, y = get(target_var))) +
    geom_smooth(method = lm, color = "blue") +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
    scale_color_gradient() +
    labs(
      title = paste(title_prefix, "Actual vs. Predicted Values"),
      x = "Predicted",
      y = "Actual"
    ) +
    theme_minimal()
  
  p2 <- ggplot(augmented_preds, aes(x = .pred, y = residual_sq)) +
    geom_point() +
    labs(title = paste(title_prefix, "Squared Residuals")) +
    theme_minimal()
  
  return(p1 | p2)
}

```

```{r Final model fitting}
# Finalize and fit the best models
bos_final_fit <- finalize_model(bos_model_params, wf_bos_tune, up_split)
osa_final_fit <- finalize_model(osa_model_params, wf_osa_tune, up_split)

# Augment predictions for evaluation
bos_aug_pred <- augment_with_residuals(bos_final_fit, up_test, "mean_percent_built_open_space")
osa_aug_pred <- augment_with_residuals(osa_final_fit, up_test, "mean_percent_open_space_access")

```

10. Final model prediction and visualization 
```{r Prediction and Visualization}
# Create prediction and residual plots for each model
bos_tuning_plots <- plot_predictions(bos_aug_pred, "mean_percent_built_open_space") + ggtitle("Built Open Space Tuning Results")
osa_tuning_plots <- plot_predictions(osa_aug_pred, "mean_percent_open_space_access") + ggtitle("Open Space Access Tuning Results")

# Combine and display the plots
combined_tuning_plots <- bos_tuning_plots / osa_tuning_plots
print(combined_tuning_plots)

```

11. Use model to predict the variables in the UN dataset for all cities with data in the Parkserve dataset.

```{r Predicting Model Outputs for full Parkserve Dataset}
# Predict for un_bos using the final BOS model
un_bos_pred <- predict(boa_final_fit, new_data = clean_parkserve_data) %>% rename(pred_mpct_built_open_space = .pred)  # Renaming the prediction column to un_bos

# Predict for un_osa using the final OSA model
un_osa_pred <- predict(osa_final_fit, new_data = clean_parkserve_data) %>% rename(pred_mpct_open_space_access = .pred)  # Renaming the prediction column to un_osa

# Join predictions back with the new_cities data
parkserve_predictions <- clean_parkserve_data %>%
  bind_cols(un_bos_pred, un_osa_pred)   # Add predicted cols
```


DOTHIS::
- Update methods from modeling section
- Check all variables for skewedness

## References
